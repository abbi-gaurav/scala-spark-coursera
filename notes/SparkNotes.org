* RDDs (Resilient Distributed Datasets)
** immutable sequence or parallel scala collection
** can be created by
*** xtf existing RDD
*** from =SparkContext= or =SparkSession=
** *Word Count E.g*
#+BEGIN_SRC scala
val rdd:RDD[String] = sparkContext.textFile("hdfs://....")
val count:RDD[(String,Int)] = rdd.flatMap(line => line.split(" "))
                                 .map(word => (word,1))
                                 .reduceByKey(_ + _)
#+END_SRC
* RDD operations (*Transformations* and *Actions*)
** Transformations
*** returns new RDDs as results
*** *Lazy*
**** allows optimizations of number of passes
*** e.g
**** map
**** flatMap
**** filter
**** distinct
*** *on 2 RDDs*
**** union
**** intersection
**** subtract
**** cartesian
** Actions (*if fn return type is not RDD*)
*** computes result based on RDD
*** returns the value or save it to *external storage system* (e.g. HDFS)
*** *Eager*
*** e.g
**** collect
**** count
**** take
**** reduce
**** foreach
*** *non-scala* e.g
**** takeSample
**** takeOrdered
**** saveAsTextFile
**** saveAsSequenceFile
* =SparkSession= or =SparkContext=
** configuration of spark job
** act as a handle to the spark cluster
** represents connection between running spark cluster and the application
** some methods to populate new RDD
*** =parallelize= : scala collection to RDD
*** =textFile= : file on *HDFS* or *local* to RDD
* Evaluation in spark
** by *default* RDDs are *computed* each time they are *evaluated*
** to *cache* RDD in memory
*** =persist()=
**** different ways
*** =cache=
**** persist with default storage level
* cluster topology
#+BEGIN_EXAMPLE



                           +------------------+
                           | Driver Program   |
                           | +--------------+ |
                           | |Spark Context | |
                           | +--------------+ |
                           +-------^----------+
                                   |
                                   |
                           +-------v-----------+
                           |Cluster Manager    |
                           |                   |
                           |                   |
                           +--------^----------+
                                    |
  +------------------+     +--------v---------+    +------------------+
  | Worker Node      |     |                  |    |                  |
  |+---------------+ |     | +--------------+ |    | +--------------+ |
  ||Executor       | |     | |              | |    | |              | |
  |+---------------+ |     | +--------------+ |    | +--------------+ |
  +------------------+     +------------------+    +------------------+

#+END_EXAMPLE
** spark programs are written w.r.t to =Driver Program=
** Driver communicate with Worker Nodes by *Cluster Manager*
** *Spark Applications*
*** set of processes running on a cluster
*** coordinated by the driver program
*** =main()= of the program runs in =driver=
*** process running the driver program 
**** creates =SparkContext=
**** creates =RDDs=
**** stages up
**** sends off transformations and actions
* Reduction Operations                                                :week2:
** foldLeft vs fold
|----------------------------------------+---------------------------------|
| foldLeft                               | fold                            |
|----------------------------------------+---------------------------------|
| ~def foldLeft[B](z:B)(f:(A,B) => B):B~ | ~def fold(z:A)(f:(A,A) => A):A~ |
| not parallelizable                     | parallelizable                  |
| signature of f stops from parallelism  |                                 |
|                                        | like a monoid                   |
** aggregate
*** ~aggregate[B](z: => B)(seqop: (B,A) => B, combop: (B.B) => B)~
*** ~seqop~ ===> foldLeft
*** ~combop~ ===> fold
*** possible to parallelize
*** change the return type of the reduction operation
** comapre to scala collections
|--------------------+-----------|
| scala collections  | spark     |
|--------------------+-----------|
| fold               | fold      |
| foldLeft/foldRight |           |
| reduce             | reduce    |
| aggregate          | aggregate |
|--------------------+-----------|
* Distributed Key value pairs (*Pair RDDs*) [[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions][PairRDDFunctions]]
** have additional methods
* xtfs
** groupByKey
** reduceByKey
~def reduceByKey(func: (V,V) => V):RDD[(K,V)]~
** mapValues
~def mapValues[U](f : V => U):RDD[(K,U)]~
** countByKey
~def countByKey():Map[K,Long]~
** keys
** Joins
*** join (Inner joins) *Lossy*
**** returns a new RDD containing combined pairs *whose keys are present in both input RDDs*
**** ~def join[W](other:RDD[(K,W)]):RDD[(K, (V,W))]~
*** leftOuterJoin/rightOuterJoin (Outer Joins)
**** returns new RDD containing combined pairs whose *keys need not be present in both input RDDs*
*** comparison
|---------------------------------------------------------------+---------------------------------------------------------------|
| leftOuterJoin                                                 | rightOuterJoin                                                |
|---------------------------------------------------------------+---------------------------------------------------------------|
| ~def leftOuterJoin[W](other:Rdd[(K,w)]):RDD[K,(V,Option[W])]~ | ~def leftOuterJoin[W](other:Rdd[(K,w)]):RDD[K,(Option[V],W)]~ |
|                                                               |                                                               |
* Actions
